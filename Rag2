import os
import asyncio
import requests
import time
from dotenv import load_dotenv
from raganything import RAGAnything, RAGAnythingConfig
import ollama

# ------------------- Load environment -------------------
load_dotenv()

USE_LOCAL = os.getenv("USE_LOCAL", "false").lower() in ("1", "true", "yes")

# Company LLM config
CLIENT_KEY   = os.getenv("CLIENT_KEY")
PASS_KEY     = os.getenv("PASS_KEY")
EMAIL        = os.getenv("EMAIL")
MODEL_ID     = os.getenv("MODEL_ID")
ENDPOINT_URL = os.getenv("ENDPOINT_URL")

# Local Ollama config
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b-q4_K_M")

# Folder for documents
DOC_FOLDER = os.getenv("DOC_FOLDER", "./docs")

# ------------------- Company LLM -------------------
def query_company_llm(prompt: str) -> str:
    headers = {
        "x-generative-ai-client": CLIENT_KEY,
        "x-openapi-token": PASS_KEY,
        "x-generative-ai-user-email": EMAIL
    }
    body = {
        "modelIds": [MODEL_ID],
        "contents": [prompt],
        "llmConfig": {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "top_k": 14,
            "top_p": 0.94,
            "temperature": 0.4,
            "repetition_penalty": 1.04
        },
        "isStream": False,
        "systemPrompt": "You are an assistant."
    }

    url = ENDPOINT_URL.rstrip("/") + "/openapi/chat/v1/models"
    for attempt in range(3):
        try:
            resp = requests.get(url, headers=headers, json=body, timeout=60)
            resp.raise_for_status()
            data = resp.json()
            if isinstance(data, dict) and "data" in data:
                return str(data["data"])
            return str(data)
        except Exception as e:
            print(f"[Company LLM Retry {attempt+1}/3] Error: {e}")
            time.sleep(2)
    return "[Error] Company LLM failed after 3 retries."

# ------------------- Local Ollama -------------------
def query_ollama(prompt: str) -> str:
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.get("message", {}).get("content", "[No content returned]")
    except Exception as e:
        return f"[ollama-error] {e}"

# ------------------- Unified LLM Interface -------------------
async def llm_model_fn(prompt: str, system_prompt: str = None, history_messages=None, **kwargs) -> str:
    text = ""
    if system_prompt:
        text += f"[system]: {system_prompt}\n"
    if history_messages:
        for m in history_messages:
            text += f"[{m.get('role', '')}]: {m.get('content', '')}\n"
    text += f"[user]: {prompt}"

    if USE_LOCAL:
        return query_ollama(text)
    else:
        return query_company_llm(text)

# ------------------- Main Entry -------------------
async def main():
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        llm_model_func=llm_model_fn,
        chunk_size=1200,  # balanced for 200-page docs
        chunk_overlap=80,
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",  # fast CPU embedding
    )

    rag = RAGAnything(config=config)

    print(f"ðŸ“‚ Ingesting from folder: {DOC_FOLDER}")
    rag.ingest_folder(DOC_FOLDER, reindex=False)  # skip re-embedding if cached
    print("âœ… Ingestion complete! Type your questions below.")

    while True:
        q = input("\nYou: ")
        if q.lower() in ("exit", "quit"):
            print("ðŸ‘‹ Exiting, bye!")
            break

        ans = await rag.query(q)
        print(f"Assistant: {ans}")

if __name__ == "__main__":
    asyncio.run(main())
